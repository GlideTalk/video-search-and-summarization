{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA AI Blueprint for Video Search and Summarization: 4xL40S Hybrid Deployment\n",
    "\n",
    "This notebook deploys the NVIDIA AI Blueprint for Video Search and Summarization using a **HYBRID APPROACH** with 4xL40S GPUs:\n",
    "- **GPU 0**: Reranker (Local)\n",
    "- **GPU 1**: Embedding (Local) \n",
    "- **GPU 2,3**: VLM - Vision Language Model (Local)\n",
    "- **Remote API**: LLM (NVIDIA Hosted)\n",
    "\n",
    "This setup provides **50% cost savings** compared to the 8xL40S setup while maintaining excellent performance.\n",
    "\n",
    "**Note**: This notebook is optimized for **4xL40S on CRUSOE Cloud Provider with Ephemeral storage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Obtain NVIDIA API Keys\n",
    "\n",
    "This key will be used to pull relevant models and containers from build.nvidia.com and NGC, plus access remote LLM services.\n",
    "\n",
    "Generate the key from [NGC Portal](https://ngc.nvidia.com/) using the same account used to apply for [Blueprint Early Access.](https://developer.nvidia.com/ai-blueprint-for-video-search-and-summarization-early-access/join)\n",
    "\n",
    "Follow the instructions to [Generate NGC API Key.](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  If you have authentication issues when pulling the NIMs, please verify you have the following <a href=\"https://org.ngc.nvidia.com/subscriptions\" target=\"_blank\">Subscriptions</a>: <strong>NVIDIA Developer Program</strong>\n",
    " </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NGC_API_KEY\"] = \"***\" #Replace with your key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the Vision Language Model (VLM)\n",
    "\n",
    "By default, we are using [VILA-1.5](https://build.nvidia.com/nvidia/vila) model. You could use other models like [NVILA](https://huggingface.co/Efficient-Large-Model/NVILA-15B), GPT-4o, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VLM_MODEL_TO_USE\"] = \"vila-1.5\" #or choose nvila\n",
    "os.environ[\"MODEL_PATH\"] = \"ngc:nim/nvidia/vila-1.5-40b:vila-yi-34b-siglip-stage3_1003_video_v8\" #for nvila, use \"git:https://huggingface.co/Efficient-Large-Model/NVILA-15B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring the user is on right path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/NVIDIA-AI-Blueprints/video-search-and-summarization.git\n",
    "# %cd video-search-and-summarization/docker/launchables\n",
    "\n",
    "%cd ./docker/launchables\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Driver and CUDA version to be the following:\n",
    "- Driver Version: 535.x.x\n",
    "- CUDA Version 12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the driver version doesn't match in the above step:\n",
    "- Update the Driver to 535\n",
    "- Reboot the system\n",
    "- Set ```NGC_API_KEY``` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt install nvidia-driver-535 -y\n",
    "# !sudo reboot now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment: 4xL40S Hybrid Configuration\n",
    "\n",
    "We will be using a hybrid approach:\n",
    "- **Local Models**: VLM, Reranker, Embedding\n",
    "- **Remote API**: LLM (saves 4 GPUs!)\n",
    "\n",
    "### GPU Configuration for 4xL40S\n",
    "\n",
    "| Component | GPU Assignment | Model |\n",
    "|-----------|---------------|---------|\n",
    "| Reranker  | GPU 0        | llama-3.2-nv-rerankqa-1b-v2 |\n",
    "| Embedding | GPU 1        | llama-3.2-nv-embedqa-1b-v2 |\n",
    "| VLM       | GPU 2,3      | VILA-1.5 or NVILA |\n",
    "| LLM       | Remote API   | llama-3.1-70b-instruct |\n",
    "\n",
    "**Cost Savings: 50% compared to 8xL40S setup!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Environment Variables and Login to Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"LOCAL_NIM_CACHE\"] = os.path.expanduser(\"~/.cache/nim\") #default cache location\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = os.path.expanduser(\"/ephemeral/cache/nim\") #updating with ephemeral storage\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the docker storage path to Ephemeral storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, subprocess, time\n",
    "\n",
    "storage_path = \"/ephemeral/cache/docker\"\n",
    "\n",
    "daemon_file = \"/etc/docker/daemon.json\" #update the path if required\n",
    "config = {}\n",
    "try:\n",
    "    config = json.load(open(daemon_file)) if os.path.exists(daemon_file) else {}\n",
    "except PermissionError:\n",
    "    print(\"Cannot read the file. Try running with elevated privileges or check docker deamon file path.\")\n",
    "\n",
    "config[\"data-root\"] = storage_path\n",
    "config_str = json.dumps(config, indent=4)\n",
    "\n",
    "subprocess.run(f\"echo '{config_str}' | sudo tee {daemon_file} > /dev/null\", shell=True, check=True)\n",
    "subprocess.run(\"sudo systemctl restart docker\", shell=True, check=True)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Verify new storage location\n",
    "print(subprocess.run(\"docker info | grep 'Docker Root Dir'\", shell=True, capture_output=True, text=True).stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: LLM NIM - SKIPPED (Using Remote API)\n",
    "\n",
    "**üöÄ COST OPTIMIZATION:** We're using remote NVIDIA API for LLM instead of local deployment.\n",
    "\n",
    "**Benefits:**\n",
    "- Saves 4 GPUs (was using GPUs 0,1,2,3)\n",
    "- Reduces infrastructure costs by 50%\n",
    "- No model download/setup time\n",
    "- Always up-to-date model\n",
    "\n",
    "The LLM API will be configured in the environment variables for the main blueprint container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ LLM NIM - SKIPPED\")\n",
    "print(\"üîÑ Using remote NVIDIA API for LLM instead\")\n",
    "print(\"üí∞ Cost savings: 4 GPUs freed up!\")\n",
    "print(\"‚ö° Faster deployment: No LLM model download needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Launch the Reranker NIM on GPU 0\n",
    "\n",
    "**Modified for 4xL40S:** Using GPU 0 instead of GPU 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    "    --gpus '\"device=0\"' \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -u $(id -u) \\\n",
    "    -p 9235:8000 \\\n",
    "    -d \\\n",
    "    nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Launch the Embedding NIM on GPU 1\n",
    "\n",
    "**Modified for 4xL40S:** Using GPU 1 instead of GPU 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    "    --gpus '\"device=1\"' \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -u $(id -u) \\\n",
    "    -p 9234:8000 \\\n",
    "    -d \\\n",
    "    nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Verify Local NIMs are Running\n",
    "\n",
    "After running the following cell, you should see **TWO containers** (Reranker and Embedding only).\n",
    "\n",
    "**Note:** No LLM container since we're using remote API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps\n",
    "print(\"\\nüéØ Expected: 2 containers (Reranker on GPU 0, Embedding on GPU 1)\")\n",
    "print(\"üìù LLM container is NOT shown - we're using remote API!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Prepare Environment for Blueprint with Hybrid Configuration\n",
    "\n",
    "Before launching the blueprint, we need to configure it for:\n",
    "- **VLM on GPUs 2,3** (local)\n",
    "- **Remote LLM API** configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update .env file to use GPUs 2,3 for VLM instead of 6,7\n",
    "import re\n",
    "\n",
    "# Read current .env file\n",
    "with open('.env', 'r') as f:\n",
    "    env_content = f.read()\n",
    "\n",
    "# Update NVIDIA_VISIBLE_DEVICES for VLM to use GPUs 2,3\n",
    "env_content = re.sub(\n",
    "    r'NVIDIA_VISIBLE_DEVICES=.*',\n",
    "    'NVIDIA_VISIBLE_DEVICES=2,3',\n",
    "    env_content\n",
    ")\n",
    "\n",
    "# Add remote LLM configuration\n",
    "remote_llm_config = '''\n",
    "# Remote LLM Configuration (4xL40S Hybrid Setup)\n",
    "USE_REMOTE_LLM=true\n",
    "REMOTE_LLM_API_BASE=https://integrate.api.nvidia.com/v1\n",
    "REMOTE_LLM_MODEL=meta/llama-3.1-70b-instruct\n",
    "'''\n",
    "\n",
    "# Add remote config if not already present\n",
    "if 'USE_REMOTE_LLM' not in env_content:\n",
    "    env_content += remote_llm_config\n",
    "\n",
    "# Write back to .env\n",
    "with open('.env', 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"‚úÖ Updated .env for 4xL40S hybrid configuration:\")\n",
    "print(\"   - VLM: GPUs 2,3\")\n",
    "print(\"   - Remote LLM API configured\")\n",
    "print(\"\\nüìã Current .env relevant settings:\")\n",
    "!grep -E \"NVIDIA_VISIBLE_DEVICES|USE_REMOTE_LLM|REMOTE_LLM\" .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, make sure you have compose.yaml, config.yaml and .env in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update docker compose version (recommended v2.32.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version\n",
    "\n",
    "!mkdir -p ~/.docker/cli-plugins\n",
    "!curl -SL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose\n",
    "!chmod +x ~/.docker/cli-plugins/docker-compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Launch the Blueprint with 4xL40S Hybrid Configuration\n",
    "\n",
    "Here, we start the docker container to spin up the blueprint. The container performs:\n",
    "1. Downloads the VLM for GPUs 2,3\n",
    "2. Performs model calibration\n",
    "3. Generates a TRT-LLM Engine for VLM\n",
    "4. Spins up the Milvus and Neo4J database\n",
    "5. Starts Video Search and Summarization service with hybrid config\n",
    "6. Finally, we get the frontend and backend endpoints\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  This step takes around 20-25 minutes for first run (faster than 8-GPU setup since no LLM download!)\n",
    "</div>\n",
    "\n",
    "The below code cell filters out important logs so that the output is not very verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!docker compose down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "keywords = [\"Milvus server started\", \"Downloading model\", \"Downloaded model\", \"VILA Embeddings\", \"VILA TRT model load execution time\", \n",
    "            \"Starting quantization\", \"Quantization done\", \"Engine generation completed\", \"TRT engines generated\", \"Uvicorn\", \n",
    "            \"VIA Server loaded\", \"Backend\", \"Frontend\", \"****\", \"Remote LLM\", \"Hybrid\"]\n",
    "\n",
    "# Start the docker compose process in detached mode\n",
    "subprocess.run(['docker', 'compose', 'up', '--quiet-pull', '-d'])\n",
    "\n",
    "def filter_logs(logs, keywords):\n",
    "    return [line for line in logs.splitlines() if any(keyword in line for keyword in keywords)]\n",
    "\n",
    "printed_lines = set()\n",
    "\n",
    "print(\"üöÄ Starting 4xL40S Hybrid VSS Deployment...\")\n",
    "print(\"üí° Configuration: Local VLM(GPUs 2,3) + Reranker(GPU 0) + Embedding(GPU 1) + Remote LLM\")\n",
    "print(\"‚è±Ô∏è  Expected time: 20-25 minutes (faster than 8-GPU setup!)\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        logs = subprocess.check_output(['docker', 'compose', 'logs', '--no-color'], universal_newlines=True)\n",
    "        filtered_logs = filter_logs(logs, keywords)\n",
    "        new_logs = [line for line in filtered_logs if line not in printed_lines]\n",
    "        \n",
    "        for line in new_logs:\n",
    "            print(line)\n",
    "            printed_lines.add(line)\n",
    "            if \"Frontend\" in line:\n",
    "                print(\"\\nüéâ VSS 4xL40S Hybrid Server ran successfully!\")\n",
    "                print(\"üí∞ Cost optimization: 50% savings compared to 8xL40S\")\n",
    "                print(\"üîó Access VSS Frontend UI from Brev portal tunnels section. Refer to Step 8 for details.\")\n",
    "                raise SystemExit\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping log tailing...\")\n",
    "except SystemExit:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Access VSS UI with Brev Tunnels\n",
    "\n",
    "1. Go to the \"Access\" Tab on Brev and scroll down to the \"Using Tunnels\" section   \n",
    "    <img src=\"images/brev_access_tab.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "2. (Optional) Add port \"9100\". This is set in .env file which is configurable   \n",
    "    <img src=\"images/brev_add_port.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "3. Click on the frontend port (9100) Sharable URL link to access blueprint UI  \n",
    "    <div class=\"alert alert-block alert-success\">\n",
    "        <b>Note:</b>  Please reload the brev page if the shareable URL is showing as unhealthy or follow the alternative steps with VSCode below.\n",
    "    </div>\n",
    "    <img src=\"images/brev_vss_ui_url.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "4. Experience VSS using the gradio based UI application with your 4xL40S hybrid setup! \n",
    "   \n",
    "   **üéØ Your Setup Performance:**\n",
    "   - Video analysis: Excellent (2 GPUs for VLM)\n",
    "   - Text processing: Fast (local Reranker & Embedding)\n",
    "   - Question answering: High-quality (remote LLM)\n",
    "   - Cost: 50% savings vs 8-GPU setup\n",
    "   \n",
    "   For quick steps to summarize a video, refer to [this link](https://docs.nvidia.com/vss/latest/content/sample_summarization.html). Additionally, for detailed instructions on how to use the UI, follow [this guide](https://docs.nvidia.com/vss/latest/content/ui_app.html)  \n",
    "    <img src=\"images/vss_landing_page.png\" alt=\"Access Tab\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Alternative Option] Access instance on VSCode and add ports\n",
    "\n",
    "1. First, go to the \"Access\" tab  \n",
    "    <img src=\"images/brev_access_tab.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "2. Refer to the commands in \"Using Brev CLI\" to open VSCode locally  \n",
    "    <div class=\"alert alert-block alert-success\">\n",
    "        <b>Note:</b>  Make sure to <a href=\"https://code.visualstudio.com/docs/setup/mac#_configure-the-path-with-vs-code\" target=\"_blank\">Configure the path with VS Code</a> if you run into erros while accessing instance through VSCode.\n",
    "    </div>  \n",
    "    <img src=\"images/brev_vscode_access.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "3. Navigate to the Ports view in the Panel region (Ports: Focus on Ports View), and select Forward a Port  \n",
    "    <img src=\"images/vscode_ports.png\" alt=\"VSCode Ports\" width=\"1200\"/>\n",
    "\n",
    "4. Add frontend (9100) and backend (8100) ports, and access blueprint UI by clicking on the \"localhost:9100\"  \n",
    "    <img src=\"images/vscode_access_vss_ui.png\" alt=\"VSCode Ports\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä 4xL40S Hybrid Setup Summary\n",
    "\n",
    "**üéØ Your Configuration:**\n",
    "- **GPU 0**: Reranker (Local) - Fast text ranking\n",
    "- **GPU 1**: Embedding (Local) - Quick vector search  \n",
    "- **GPU 2,3**: VLM (Local) - Excellent video analysis\n",
    "- **Remote**: LLM - High-quality responses\n",
    "\n",
    "**üí∞ Cost Benefits:**\n",
    "- 50% reduction in GPU costs\n",
    "- Faster deployment (no LLM download)\n",
    "- Same video analysis quality\n",
    "\n",
    "**‚ö° Performance:**\n",
    "- Video understanding: Excellent (2 dedicated GPUs)\n",
    "- Search & ranking: Fast (local models)\n",
    "- Q&A responses: High-quality (remote LLM)\n",
    "\n",
    "Congratulations on your optimized video search and summarization setup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uninstalling the blueprint\n",
    "\n",
    "Uncomment the following cell to stop the blueprint container, followed by stopping all model containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # To bring down the blueprint instance\n",
    "# !docker compose down\n",
    "\n",
    "# # To stop all other containers (Reranker and Embedding)\n",
    "# !docker stop $(docker ps -q)\n",
    "\n",
    "# print(\"üõë All VSS components stopped\")\n",
    "# print(\"üí° Your 4xL40S hybrid configuration is saved and ready to restart anytime!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps\n",
    "print(\"\\nüìä Expected containers for 4xL40S setup:\")\n",
    "print(\"   ‚Ä¢ VSS main container (blueprint)\")\n",
    "print(\"   ‚Ä¢ Reranker NIM (GPU 0)\")\n",
    "print(\"   ‚Ä¢ Embedding NIM (GPU 1)\")\n",
    "print(\"   ‚Ä¢ Database containers (Milvus, Neo4j)\")\n",
    "print(\"\\nüö´ NOT running locally: LLM (using remote API)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}